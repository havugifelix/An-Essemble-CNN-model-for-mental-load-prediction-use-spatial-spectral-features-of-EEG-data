{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2,l1\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "# from keras.regularizers import regularizers\n",
    "from numpy import load\n",
    "from  keras.preprocessing.image import DirectoryIterator\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_alpha=\"/home/kashraf/felix_hd/topomap+csv_entire/topomap/alpha/train/\"\n",
    "train_path_beta=\"/home/kashraf/felix_hd/topomap+csv_entire/topomap/beta/train/\"\n",
    "train_path_theta= \"/home/kashraf/felix_hd/topomap+csv_entire/topomap/theta/train/\"\n",
    "\n",
    "test_path_alpha=\"/home/kashraf/felix_hd/topomap+csv_entire/topomap/alpha/test/\"\n",
    "test_path_theta=\"/home/kashraf/felix_hd/topomap+csv_entire/topomap/theta/test/\"\n",
    "test_path_beta= \"/home/kashraf/felix_hd/topomap+csv_entire/topomap/beta/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30240 images belonging to 4 classes.\n",
      "Found 30240 images belonging to 4 classes.\n",
      "Found 30240 images belonging to 4 classes.\n",
      "Found 13760 images belonging to 4 classes.\n",
      "Found 13760 images belonging to 4 classes.\n",
      "Found 13760 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator( rescale=1./255,\n",
    "      rotation_range=0,\n",
    "      width_shift_range=0,\n",
    "      height_shift_range=0,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "test_datagen=ImageDataGenerator( rescale=1./255)\n",
    "\n",
    "train_data=[train_datagen.flow_from_directory(train_path_theta,batch_size=32,shuffle=False),\n",
    "         train_datagen.flow_from_directory(train_path_alpha,batch_size=32,shuffle=False),\n",
    "         train_datagen.flow_from_directory(train_path_beta,batch_size=32,shuffle=False)]\n",
    "\n",
    "test_data=[test_datagen.flow_from_directory(test_path_theta,shuffle=False),\n",
    "          test_datagen.flow_from_directory(test_path_alpha,shuffle=False),\n",
    "          test_datagen.flow_from_directory(test_path_beta,shuffle=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lett's build our essemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8c63d506b97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models built and compiled\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 96)        55392     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 128)       110720    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "mbm (Dense)                  (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 5,273,508\n",
      "Trainable params: 5,271,844\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "num_classes=4\n",
    "nets = 3\n",
    "img_rows=256\n",
    "img_cols=256\n",
    "model = [0] *nets\n",
    "\n",
    "for j in range(nets):\n",
    "    model[j]= Sequential()\n",
    " # First CONV-ReLU Layer\n",
    "    model[j].add(Conv2D(32, (3, 3), padding = 'same', input_shape = (img_rows, img_cols,3)))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Activation('relu'))\n",
    "    model[j].add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Second CONV-ReLU Layer\n",
    "    model[j].add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Activation('relu'))\n",
    "    model[j].add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model[j].add(Conv2D(96, (3, 3), padding=\"same\"))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Activation('relu'))\n",
    "    model[j].add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model[j].add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Activation('relu'))\n",
    "    model[j].add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model[j].add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Activation('relu'))\n",
    "    model[j].add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model[j].add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Activation('relu'))\n",
    "    model[j].add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model[j].add(Flatten())\n",
    "\n",
    "    model[j].add(Dense(1024))\n",
    "    model[j].add(Activation('relu'))\n",
    "    model[j].add(Dropout(0.4))\n",
    "\n",
    "\n",
    "    # Final Dense Layer\n",
    "    model[j].add(Dense(num_classes,activation='softmax',name='mbm'))\n",
    "    model[j].compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.0001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(\"All models built and compiled\")\n",
    "print(model[0].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "945/945 [==============================] - 1562s 2s/step - loss: 0.9625 - accuracy: 0.6520 - val_loss: 0.2101 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21008, saving model to /home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_theta--2.h5\n",
      "Epoch 2/60\n",
      "945/945 [==============================] - 1559s 2s/step - loss: 0.1869 - accuracy: 0.9310 - val_loss: 0.0210 - val_accuracy: 0.9217\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21008 to 0.02098, saving model to /home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_theta--2.h5\n",
      "Epoch 3/60\n",
      "945/945 [==============================] - 1558s 2s/step - loss: 0.0713 - accuracy: 0.9762 - val_loss: 0.0043 - val_accuracy: 0.9698\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02098 to 0.00429, saving model to /home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_theta--2.h5\n",
      "Epoch 4/60\n",
      "945/945 [==============================] - 1558s 2s/step - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.0083 - val_accuracy: 0.9640\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00429\n",
      "Epoch 5/60\n",
      "945/945 [==============================] - 1558s 2s/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 0.0672 - val_accuracy: 0.9052\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00429\n",
      "Epoch 6/60\n",
      "945/945 [==============================] - 1558s 2s/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 1.0804e-04 - val_accuracy: 0.9778\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00429 to 0.00011, saving model to /home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_theta--2.h5\n",
      "Epoch 7/60\n",
      "945/945 [==============================] - 1559s 2s/step - loss: 0.0561 - accuracy: 0.9838 - val_loss: 2.9979e-04 - val_accuracy: 0.9870\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00011\n",
      "Epoch 8/60\n",
      "945/945 [==============================] - 1558s 2s/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 1.3972e-04 - val_accuracy: 0.9812\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00011\n",
      "Epoch 9/60\n",
      "945/945 [==============================] - 1558s 2s/step - loss: 9.5871e-04 - accuracy: 0.9998 - val_loss: 4.4710e-05 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00011 to 0.00004, saving model to /home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_theta--2.h5\n",
      "Epoch 10/60\n",
      "945/945 [==============================] - 1556s 2s/step - loss: 0.0729 - accuracy: 0.9791 - val_loss: 0.0012 - val_accuracy: 0.9869\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00004\n",
      "Epoch 11/60\n",
      "945/945 [==============================] - 1556s 2s/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0011 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00004\n",
      "Epoch 12/60\n",
      "945/945 [==============================] - 1556s 2s/step - loss: 0.0661 - accuracy: 0.9803 - val_loss: 0.0012 - val_accuracy: 0.9863\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00004\n",
      "Epoch 13/60\n",
      "945/945 [==============================] - 1555s 2s/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 2.2011e-04 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 14/60\n",
      "945/945 [==============================] - 1555s 2s/step - loss: 4.7908e-04 - accuracy: 1.0000 - val_loss: 8.7178e-05 - val_accuracy: 0.9914\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00004\n",
      "Epoch 15/60\n",
      "651/945 [===================>..........] - ETA: 7:12 - loss: 4.0462e-04 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "# batch_size=16\n",
    "epochs=60\n",
    "checkpoint = [ModelCheckpoint(\"/home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_theta--2.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1),\n",
    "               ModelCheckpoint(\"/home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_alpha--2.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1),\n",
    "               ModelCheckpoint(\"/home/kashraf/Felix_work2/2021_research/essemble_models/essembel_sub_beta--2.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)]\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 4,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.000001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks =[ [ checkpoint[0],reduce_lr],[ checkpoint[1],reduce_lr],[ checkpoint[2],reduce_lr]]\n",
    "\n",
    "\n",
    "history =[0]*nets \n",
    "for i in range(nets):\n",
    "    history[i] = model[i].fit_generator(train_data[i], \n",
    "                                         epochs = epochs,validation_data =test_data[i],\n",
    "                                         callbacks=callbacks[i], verbose=1)\n",
    "    \n",
    "# model.save(\"/home/kashraf/Felix_work2/modelsDL/stack_AlexNet_1_Epoch.h5\")\n",
    "\n",
    "# Evaluate the performance of our trained model\n",
    "score_alpha= model[0].evaluate(x_test[0], y_test[0], verbose=1)\n",
    "score_theta= model[1].evaluate(x_test[1], y_test[1], verbose=1)\n",
    "score_beta= model[2].evaluate(x_test[2], y_test[2], verbose=1)\n",
    "\n",
    "print('Test loss theta:', score_theta[0])\n",
    "print('Test accuracy theta: ', score_theta[1])\n",
    "\n",
    "print('Test loss alpha:', score_alpha[0])\n",
    "print('Test accuracy alpha :', score_alpha[1])\n",
    "\n",
    "print('Test loss beta:', score_beta[0])\n",
    "print('Test accuracy beta:', score_beta[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put our data in the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, random_state=4)\n",
    "\n",
    "x_train = x_train.astype('float16')\n",
    "x_test = x_test.astype('float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=4)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our pre_trained Bany's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "num_classes=4\n",
    "\n",
    "# First CONV-ReLU Layer\n",
    "model.add(Conv2D(32, (7, 7), padding = 'same', input_shape = (img_rows, img_cols,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second CONV-ReLU Layer\n",
    "model.add(Conv2D(64, (5, 5), padding = \"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(96, (3, 3), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\",kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "# Final Dense Layer\n",
    "model.add(Dense(num_classes,activation='softmax',name='mbm'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "checkpoint = ModelCheckpoint(\"/home/kashraf/Felix_work2/modelsDL/theta_topomap-weights--2.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 4,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 4,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.000001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [ checkpoint,reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.0001),\n",
    "              metrics = ['accuracy'])\n",
    "batch_size=16\n",
    "epochs=100\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          callbacks = callbacks,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "# model.save(\"/home/kashraf/Felix_work2/modelsDL/stack_AlexNet_1_Epoch.h5\")\n",
    "\n",
    "# Evaluate the performance of our trained model\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our model history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle_out = open(\"beta_history.pickle\",\"wb\")\n",
    "pickle.dump(history.history, pickle_out)\n",
    "pickle_out.close()\n",
    "# pickle_in = open(\"alphaAlexnet_history.pickle\",\"rb\")\n",
    "# saved_history = pickle.load(pickle_in)\n",
    "# print(saved_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting our loss charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')\n",
    "line2 = plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
    "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting our accuracy charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\n",
    "line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n",
    "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
    "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred))\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency Visualizations\n",
    "To visualize activation over final dense layer outputs, we need to switch the softmax activation out for linear since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide utils.apply_modifications to modify network parameters and rebuild the graph.\n",
    "\n",
    "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear' and compare what happens if we dont do this at the end.\n",
    "\n",
    "Lets pick an input over which we want to show the attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_saliency\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class_idx = 0\n",
    "indices = np.where(y_test[:, class_idx] == 1.)[0]\n",
    "\n",
    "# pick some random input from here.\n",
    "idx = indices[0]\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx =-1\n",
    "\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "grads = visualize_saliency(model, layer_idx, filter_indices=class_idx, seed_input=x_test[idx])\n",
    "# Plot with 'jet' colormap to visualize as a heatmap.\n",
    "plt.imshow(grads, cmap='RdBu_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To used guided saliency, we need to set backprop_modifier='guided'. For rectified saliency or deconv saliency, use backprop_modifier='relu'. Lets try these options quickly and see how they compare to vanilla saliency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modifier in ['guided', 'relu']:\n",
    "    grads = visualize_saliency(model, layer_idx, filter_indices=class_idx,\n",
    "                               seed_input=x_test[idx], backprop_modifier=modifier)\n",
    "    plt.figure()\n",
    "    plt.title(modifier)\n",
    "    plt.imshow(grads, cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This corresponds to the Dense linear layer.\n",
    "for class_idx in np.arange(4):    \n",
    "    indices = np.where(y_test[:, class_idx] == 1.)[0]\n",
    "    idx = indices[0]\n",
    "    \n",
    "    plt.figure(figsize=(25,15))\n",
    "    f, ax = plt.subplots(1, 4)\n",
    "   \n",
    "    ax[0].imshow(x_test[idx][..., 0])\n",
    "    \n",
    "    for i, modifier in enumerate([None, 'guided', 'relu']):\n",
    "        grads = visualize_saliency(model, layer_idx, filter_indices=class_idx, \n",
    "                                   seed_input=x_test[idx], backprop_modifier=modifier)\n",
    "        if modifier is None:\n",
    "            modifier = 'vanilla'\n",
    "        ax[i+1].set_title(modifier)    \n",
    "        ax[i+1].imshow(grads, cmap='jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
